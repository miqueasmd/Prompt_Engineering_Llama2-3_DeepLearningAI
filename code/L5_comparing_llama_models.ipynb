{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde90571-0799-48af-b315-247f85c2269b",
   "metadata": {},
   "source": [
    "# Comparing Llama Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce53734-47df-454d-bb29-c135b2a1b338",
   "metadata": {},
   "source": [
    "**Update: Llama 3 was released on April 18 and this notebook has been updated to compare Llama 3 and Llama 2 models hosted on Together.ai.**\n",
    "\n",
    "- Load helper function to prompt Llama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469f76b7-acd9-4192-93f2-a37d4ac9d196",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import llama, llama_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd039f-d768-44a7-b81b-bfab14a33a90",
   "metadata": {},
   "source": [
    "### Task 1: Sentiment Classification\n",
    "- Compare the models on few-shot prompt sentiment classification.\n",
    "- You are asking the model to return a one word response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d44286-dd86-48bb-9457-72dca64b5b37",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: Positive\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "Message: Can't wait to order pizza for dinner tonight!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38108527-032d-411b-bb3e-31471694758f",
   "metadata": {},
   "source": [
    "- First, use the 7B parameter chat model (`llama-2-7b-chat`) to get the response.\n",
    "\n",
    "**Note the model names accepted by Together.ai are case insensitive and can be either \"META-LLAMA/LLAMA-2-7B-CHAT-HF\" or \"togethercomputer/llama-2-7b-chat\". The names starting with \"META-LLAMA\" are preferred now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58a65f2-a8e2-4cf4-abc4-c65fd64710c9",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hungry\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 #model=\"togethercomputer/LLama-2-7b-chat\")\n",
    "                 model = \"META-LLAMA/Llama-2-7B-CHAT-HF\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24729f9d-0899-472b-9025-12824b962a52",
   "metadata": {},
   "source": [
    "- Now, use the 70B parameter chat model (`Qwen/QwQ-32B-Preview`) on the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f62ef43-0b6a-4c11-bed9-8f49ec4f0802",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "Negative\n",
      "Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 #model=\"Qwen/QwQ-32B-Preview\")\n",
    "                 model=\"Qwen/QwQ-32B-Preview\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4a50d-d872-4707-996b-885a2e310915",
   "metadata": {},
   "source": [
    "**Using Llama 3 chat models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cc8e6e8-986e-47e6-8691-1f0aabcea1ea",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excited\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 model = \"META-LLAMA/Llama-3-8B-CHAT-HF\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e2842f4-5c9c-4796-a012-1e4a5a7c205c",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 model = \"META-LLAMA/Llama-3-70B-CHAT-HF\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c5ded-3bbe-4cac-898c-578655ea47cc",
   "metadata": {},
   "source": [
    "### Task 2: Summarization\n",
    "- Compare the models on summarization task.\n",
    "- This is the same \"email\" as the one you used previously in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07d248f0-a610-4696-aacd-12ca94b2fb8c",
   "metadata": {
    "height": 574
   },
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear Amit,\n",
    "\n",
    "An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n",
    "\n",
    "Here are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n",
    "\n",
    "Prompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\n",
    "One-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task â€” the input and the desired output â€” sometimes yields better results.\n",
    "Fine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\n",
    "Pretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\n",
    "For most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If youâ€™re unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesnâ€™t work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesnâ€™t deliver the performance you want, then try fine-tuning â€” but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n",
    "\n",
    "(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? ðŸ˜œ)\n",
    "\n",
    "Additional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, thatâ€™s not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLMâ€™s output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning â€” in which GPT-4 surpasses current open models â€” it can be difficult to fine-tune a smaller model to deliver superior results.\n",
    "\n",
    "Beyond choosing a development approach, itâ€™s also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. Iâ€™ll talk about how to make this choice in a future letter.\n",
    "\n",
    "Keep learning!\n",
    "\n",
    "Andrew\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize this email and extract some key points.\n",
    "\n",
    "What did the author say about llama models?\n",
    "```\n",
    "{email}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f00bb-51d9-492a-920b-5b286ab5f5c4",
   "metadata": {},
   "source": [
    "- First, use the 7B parameter chat model (`llama-2-7b-chat`) to summarize the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99c60da6-9b3e-4f11-a1d5-8e5e273e91ac",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The author of the email discusses the use of large language models (LLMs) and how they can be used to build applications. The email highlights several ways to build applications based on LLMs, ranging from simple prompting to fine-tuning.\n",
      "\n",
      "Key points from the email include:\n",
      "\n",
      "1. Increasing variety of LLMs are open source or close to it, giving developers more options for building applications.\n",
      "2. Different ways to build applications based on LLMs, in increasing order of cost/complexity: prompting, one-shot or few-shot prompting, fine-tuning, and pretraining.\n",
      "3. For most teams, starting with prompting is recommended, as it allows for quick development of an application.\n",
      "4. If unsatisfied with the quality of the output, developers can ease into more complex techniques gradually.\n",
      "5. Fine-tuning a smaller model can work well for changing the style of an LLM's output, but may not yield superior results than prompting a larger, more capable model.\n",
      "6. Choosing a specific model also needs to be considered, as smaller models require less processing power and tend to have more knowledge about the world and better reasoning ability.\n",
      "7. The author mentions that a member of the DeepLearning.AI team has been trying to fine-tune a model called Llama-2-7B to sound like them, which they find useful for understanding the capabilities of these models.\n",
      "\n",
      "Overall, the email provides an overview of the different approaches to building applications using LLMs and highlights the importance of choosing the right development approach and model for the specific application.\n"
     ]
    }
   ],
   "source": [
    "response_7b = llama(prompt,\n",
    "                model=\"META-LLAMA/Llama-2-7B-CHAT-HF\")\n",
    "print(response_7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e84506c-c6b9-4512-9fbc-1e27140d4a2f",
   "metadata": {},
   "source": [
    "- Now, use the 13B parameter chat model (`meta-llama/Llama-Vision-Free`) to summarize the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6af5fb7-8538-4831-bf02-a18474b04c61",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary:**\n",
      "The email discusses the increasing availability of large language models (LLMs) and provides guidance on how to build applications using these models. The author, Andrew, recommends starting with prompting, which allows for quick prototyping, and gradually moving to more complex techniques like fine-tuning if needed. He also touches on the importance of choosing the right model for the application and the trade-offs between smaller and larger models.\n",
      "\n",
      "**Key points:**\n",
      "\n",
      "* LLMs are becoming more open-source, giving developers more options for building applications.\n",
      "* There are different ways to build applications using LLMs, including prompting, one-shot or few-shot prompting, fine-tuning, and pretraining.\n",
      "* Prompting is a good starting point, as it allows for quick prototyping without requiring a large training set.\n",
      "* One-shot or few-shot prompting can improve results by providing examples of how to carry out a task.\n",
      "* Fine-tuning is a more complex technique that requires a small dataset and can lead to better results, but may require hundreds or thousands more examples.\n",
      "* Choosing the right model is also important, with smaller models requiring less processing power but larger models having more knowledge and better reasoning ability.\n",
      "* The author recommends starting with prompting and gradually moving to more complex techniques if needed, and encourages developers to take a course on Generative AI with Large Language Models to gain a deeper understanding of these options.\n"
     ]
    }
   ],
   "source": [
    "response_free = llama(prompt,\n",
    "                model=\"meta-llama/Llama-Vision-Free\")\n",
    "print(response_free)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa0c61-ff78-4372-b4fb-a51be7bf11eb",
   "metadata": {},
   "source": [
    "- Lastly, use the 70B parameter chat model (`Qwen/QwQ-32B-Preview`) to summarize the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a031c7e-d4d0-404d-867c-27f8a0ab96eb",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This email is from Andrew, likely Andrew Ng, a well-known figure in the field of artificial intelligence and machine learning. He is addressing someone named Amit and discussing the growing availability of large language models (LLMs), particularly those that are open-source or have permissive licenses. Andrew emphasizes the benefits of this trend for developers looking to build applications using LLMs.\n",
      "\n",
      "He outlines four methods for building applications based on LLMs, arranged in order of increasing cost and complexity:\n",
      "\n",
      "1. **Prompting**: This involves providing pre-trained LLMs with instructions to create prototypes quickly, often in minutes or hours, without needing a training dataset. Andrew mentions that many people have been experimenting with this approach and that his short courses teach best practices for it.\n",
      "\n",
      "2. **One-shot or few-shot prompting**: Beyond simple prompting, this method includes giving the LLM a few examples of inputs and desired outputs to improve performance.\n",
      "\n",
      "3. **Fine-tuning**: This involves further training a pre-trained LLM on a small dataset specific to the task at hand. Andrew notes that tools for fine-tuning are becoming more mature, making it more accessible to developers.\n",
      "\n",
      "4. **Pretraining**: This is the most resource-intensive approach, involving training an LLM from scratch. It's rarely done due to the high computational requirements, except for creating specialized models like BloombergGPT for finance or Med-PaLM 2 for medicine.\n",
      "\n",
      "Andrew recommends starting with prompting for quick results and gradually moving to more complex methods if needed, such as one-shot prompting, RAG (retrieval augmented generation), or fine-tuning. He also mentions a course called \"Generative AI with Large Language Models\" by AWS and DeepLearning.AI for those who want to delve deeper into these topics.\n",
      "\n",
      "There's an amusing aside about a team member attempting to fine-tune the Llama-2-7B model to sound like Andrew, which leads him to wonder about the security of his job.\n",
      "\n",
      "Additionally, Andrew touches on the complexities involved when developers want to move from prompting a proprietary model like GPT-4 to fine-tuning a smaller model. He suggests that the decision between these approaches depends on the specific application, such as whether the goal is to change the style of output or to perform complex reasoning.\n",
      "\n",
      "Finally, he mentions that choosing the right model size is another important consideration, with larger models offering more knowledge and better reasoning abilities but requiring more processing power. He hints at discussing this choice in a future communication.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- Open-source and permissively licensed LLMs are becoming more available, providing developers with more options.\n",
      "\n",
      "- Four methods for building applications with LLMs are outlined, ranging from simple prompting to pretraining from scratch.\n",
      "\n",
      "- Andrew recommends starting with the simplest method (prompting) and gradually increasing complexity as needed.\n",
      "\n",
      "- He mentions a course on generative AI with LLMs for those seeking to learn more.\n",
      "\n",
      "- There are considerations and trade-offs when moving from prompting proprietary models to fine-tuning smaller models.\n",
      "\n",
      "- Choosing the right model size involves balancing knowledge and reasoning capabilities with computational requirements.\n",
      "\n",
      "**Regarding Llama Models:**\n",
      "\n",
      "- Andrew mentions that a team member is attempting to fine-tune the Llama-2-7B model to sound like him, indicating that Llama models are being used and experimented with within his organization.\n",
      "\n",
      "- This suggests that Llama models are considered versatile and customizable, as they can be fine-tuned to mimic specific styles or personalities.\n",
      "\n",
      "- The fact that a smaller model like Llama-2-7B is being fine-tuned implies that it is computationally feasible for many developers, making it an attractive option compared to larger models that require more resources.\n",
      "\n",
      "**Overall, the email highlights the evolving landscape of LLMs, the various strategies for leveraging them in applications, and the need to make informed choices based on specific requirements and constraints.**\n"
     ]
    }
   ],
   "source": [
    "response_qwen = llama(prompt,\n",
    "                model=\"Qwen/QwQ-32B-Preview\")\n",
    "print(response_qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0925c56a-7fb9-46ba-9680-8dc489cc0ce7",
   "metadata": {},
   "source": [
    "**Using Llama 3 chat models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17331039-712d-444d-b0c8-1dac330cd6c1",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the email and some key points:\n",
      "\n",
      "**Summary:** The author, Andrew, discusses the various ways to build applications using large language models (LLMs), including prompting, one-shot or few-shot prompting, fine-tuning, and pretraining. He recommends starting with prompting and gradually moving to more complex techniques if needed.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* LLMs with permissive licenses provide developers with more options for building applications.\n",
      "* Prompting is a simple and fast way to build a prototype, but may not yield high-quality results.\n",
      "* One-shot or few-shot prompting can improve results by providing examples of desired outputs.\n",
      "* Fine-tuning an LLM requires a small dataset and more resources, but can lead to better results.\n",
      "* Pretraining an LLM from scratch is resource-intensive and typically not done by most teams.\n",
      "* The author recommends starting with prompting and gradually moving to more complex techniques if needed.\n",
      "* Choosing the right model (smaller vs. larger) is also important, depending on the application and desired results.\n",
      "\n",
      "As for llama models, the author mentions that a member of the DeepLearning.AI team is trying to fine-tune Llama-2-7B to sound like him, but does not provide further information about llama models or their capabilities.\n"
     ]
    }
   ],
   "source": [
    "response_llama3_8b = llama(prompt,\n",
    "                model=\"META-LLAMA/Llama-3-8B-CHAT-HF\")\n",
    "print(response_llama3_8b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c7aa9e3-819d-471b-a776-e5e37265e326",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the email and some key points:\n",
      "\n",
      "**Summary:** The email discusses the increasing availability of open-source large language models (LLMs) and the various ways to build applications using them, ranging from simple prompting to fine-tuning and pretraining. The author recommends starting with prompting and gradually moving to more complex techniques if needed.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "1. Open-source LLMs are becoming more available, giving developers more options for building applications.\n",
      "2. There are four ways to build applications using LLMs, in increasing order of cost/complexity: prompting, one-shot or few-shot prompting, fine-tuning, and pretraining.\n",
      "3. Prompting is a quick and easy way to build a prototype, while fine-tuning and pretraining require more resources and expertise.\n",
      "4. The choice of development approach depends on the application and the desired output.\n",
      "5. The author recommends starting with prompting and gradually moving to more complex techniques if needed.\n",
      "6. The choice of model size is also important, with smaller models requiring less processing power but larger models having more knowledge and better reasoning ability.\n",
      "7. The author mentions a fun fact about someone trying to fine-tune a llama model to sound like them, raising the question of job security in the age of AI.\n"
     ]
    }
   ],
   "source": [
    "response_llama3_70b = llama(prompt,\n",
    "                model=\"META-LLAMA/Llama-3-70B-CHAT-HF\")\n",
    "print(response_llama3_70b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f051d-a7e4-45cc-b162-646862084994",
   "metadata": {},
   "source": [
    "#### Model-Graded Evaluation: Summarization\n",
    "\n",
    "- Interestingly, you can ask a LLM to evaluate the responses of other LLMs.\n",
    "- This is known as **Model-Graded Evaluation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77de7b-7339-4ff3-b8fd-63cb448d4ed8",
   "metadata": {},
   "source": [
    "- Create a `prompt` that will evaluate these three responses using 70B parameter chat model (`llama-2-70b-chat`).\n",
    "- In the `prompt`, provide the \"email\", \"name of the models\", and the \"summary\" generated by each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e10e1d72-10ab-43dd-80f8-74fcf57e28a4",
   "metadata": {
    "height": 608
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the evaluation of each model's summary:\n",
      "\n",
      "**Model: llama-2-7b-chat**\n",
      "\n",
      "* The summary is concise and covers the main points of the original email, but it lacks details and specific examples.\n",
      "* The summary follows the instructions of the prompt, but it's too brief and doesn't provide much insight into the original email.\n",
      "* The model's output is straightforward and lacks creativity.\n",
      "\n",
      "**Model: llama-2-13b-chat**\n",
      "\n",
      "* The summary is extremely brief and doesn't provide any meaningful information about the original email.\n",
      "* The summary doesn't follow the instructions of the prompt, as it's not a summary of the email.\n",
      "* The model's output is unhelpful and doesn't demonstrate any understanding of the original email.\n",
      "\n",
      "**Model: Qwen/QwQ-32B-Preview**\n",
      "\n",
      "* The summary is detailed and covers all the main points of the original email, including specific examples and nuances.\n",
      "* The summary follows the instructions of the prompt and provides a clear and concise overview of the email.\n",
      "* The model's output is well-structured and demonstrates a good understanding of the original email.\n",
      "* The summary also provides additional insights and analysis, such as the mention of Llama models being versatile and customizable.\n",
      "\n",
      "**Model: llama-3-8b-chat**\n",
      "\n",
      "* The summary is concise and covers the main points of the original email, but it lacks specific examples and details.\n",
      "* The summary follows the instructions of the prompt, but it's too brief and doesn't provide much insight into the original email.\n",
      "* The model's output is straightforward and lacks creativity.\n",
      "\n",
      "**Model: llama-3-70b-chat**\n",
      "\n",
      "* The summary is concise and covers the main points of the original email, but it lacks specific examples and details.\n",
      "* The summary follows the instructions of the prompt, but it's too brief and doesn't provide much insight into the original email.\n",
      "* The model's output is straightforward and lacks creativity.\n",
      "\n",
      "Based on the evaluation, I would recommend the **Qwen/QwQ-32B-Preview** model as the best performer. Its summary is detailed, well-structured, and demonstrates a good understanding of the original email. It also provides additional insights and analysis, making it a valuable summary.\n",
      "\n",
      "The other models, while they may have some strengths, lack the detail and insight provided by the Qwen/QwQ-32B-Preview model. The llama-2-13b-chat model is particularly weak, as its summary is extremely brief and doesn't provide any meaningful information about the original email.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the original text denoted by `email`\n",
    "and the name of several models: `model:<name of model>\n",
    "as well as the summary generated by that model: `summary`\n",
    "\n",
    "Provide an evaluation of each model's summary:\n",
    "- Does it summarize the original text well?\n",
    "- Does it follow the instructions of the prompt?\n",
    "- Are there any other interesting characteristics of the model's output?\n",
    "\n",
    "Then compare the models based on their evaluation \\\n",
    "and recommend the models that perform the best.\n",
    "\n",
    "email: ```{email}`\n",
    "\n",
    "model: llama-2-7b-chat\n",
    "summary: {response_7b}\n",
    "\n",
    "model: meta-llama/Llama-Vision-Free\n",
    "summary: {response_free}\n",
    "\n",
    "model: Qwen/QwQ-32B-Preview\n",
    "summary: {response_qwen}\n",
    "\n",
    "model: llama-3-8b-chat\n",
    "summary: {response_llama3_8b}\n",
    "\n",
    "model: llama-3-70b-chat\n",
    "summary: {response_llama3_70b}\n",
    "\"\"\"\n",
    "\n",
    "response_eval = llama(prompt,\n",
    "                model=\"META-LLAMA/Llama-3-70B-CHAT-HF\")\n",
    "print(response_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a5581-bb28-4eba-be19-045be0779ed0",
   "metadata": {},
   "source": [
    "### Task 3: Reasoning ###\n",
    "- Compare the three models' performance on reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f98a66a-6d17-4a6d-bae8-881a5d5a1d4d",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Jeff and Tommy are neighbors\n",
    "\n",
    "Tommy and Eddy are not neighbors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff5198c1-701d-4e07-93a1-38ab7a12e07c",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Are Jeff and Eddy neighbors?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da3a23b9-0dde-4cc2-83cc-b9344b5983d4",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given this context: ```{context}```,\n",
    "\n",
    "and the following query:\n",
    "```{query}```\n",
    "\n",
    "Please answer the questions in the query and explain your reasoning.\n",
    "If there is not enough informaton to answer, please say\n",
    "\"I do not have enough information to answer this questions.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d63881-485c-4170-9bb0-a935ea79e4de",
   "metadata": {},
   "source": [
    "- First, use the 7B parameter chat model (`llama-2-7b-chat`) for the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "544f3d8b-14e4-497b-9a3e-3bcc9f9f623c",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, I'd be happy to help! Based on the information provided, we can answer the query as follows:\n",
      "\n",
      "Are Jeff and Eddy neighbors?\n",
      "\n",
      "No, Jeff and Eddy are not neighbors. The context states that Jeff and Tommy are neighbors, but does not mention Eddy as a neighbor. Therefore, we can conclude that Eddy and Jeff are not neighbors.\n"
     ]
    }
   ],
   "source": [
    "response_7b_chat = llama(prompt,\n",
    "                        model=\"META-LLAMA/Llama-2-7B-CHAT-HF\")\n",
    "print(response_7b_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66b3dc-a373-4c30-8554-9ef855a076c2",
   "metadata": {},
   "source": [
    "- Now, use the 13B parameter chat model (`meta-llama/Llama-Vision-Free`) for the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fa0f0e9-d26e-4736-a053-abe175b4d5f7",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have enough information to answer this question.\n",
      "\n",
      "The context only provides information about the relationships between Jeff, Tommy, and Eddy, but it does not provide any information about the relationship between Jeff and Eddy specifically.\n"
     ]
    }
   ],
   "source": [
    "response_free_chat = llama(prompt,\n",
    "                        model=\"meta-llama/Llama-Vision-Free\")\n",
    "print(response_free_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d7dc11-c961-4307-ba92-d0b88b9d8fc3",
   "metadata": {},
   "source": [
    "- Then, use the 70B parameter chat model (`Qwen/QwQ-32B-Preview`) for the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3477846-ae26-4411-afa8-747261f0d6f9",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have enough information to answer this question.\n",
      "\n",
      "Explanation:\n",
      "\n",
      "From the given context, we know two things:\n",
      "\n",
      "1. Jeff and Tommy are neighbors.\n",
      "\n",
      "2. Tommy and Eddy are not neighbors.\n",
      "\n",
      "The question is whether Jeff and Eddy are neighbors.\n",
      "\n",
      "Let's think about this step by step.\n",
      "\n",
      "First, since Jeff and Tommy are neighbors, they live next to each other.\n",
      "\n",
      "Second, Tommy and Eddy are not neighbors, so Eddy does not live next to Tommy.\n",
      "\n",
      "However, there is no direct information about the relationship between Jeff's and Eddy's residences.\n",
      "\n",
      "It's possible that Jeff and Eddy are neighbors, especially if Eddy lives next to Jeff, skipping Tommy.\n",
      "\n",
      "Alternatively, Eddy could live farther away, not being neighbors with either Jeff or Tommy.\n",
      "\n",
      "Without more information about the layout of their homes or Eddy's location relative to Jeff, we cannot definitively say whether Jeff and Eddy are neighbors.\n",
      "\n",
      "Therefore, I do not have enough information to answer this question.\n",
      "\n",
      "**Final Answer**\n",
      "\n",
      "\\[ \\boxed{\\text{I do not have enough information to answer this question.}} \\]\n"
     ]
    }
   ],
   "source": [
    "response_qwen_chat = llama(prompt,\n",
    "                        model=\"Qwen/QwQ-32B-Preview\")\n",
    "print(response_qwen_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce6f061-1040-42d5-81b1-e37709445c1e",
   "metadata": {},
   "source": [
    "- Lastly, use the Llama 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f958db46-abe8-4518-8db4-8829f00cde61",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, I can answer the query as follows:\n",
      "\n",
      "Are Jeff and Eddy neighbors?\n",
      "\n",
      "I do not have enough information to answer this question.\n",
      "\n",
      "The context only provides information about Jeff and Tommy being neighbors, and Tommy and Eddy not being neighbors. It does not provide any information about Jeff and Eddy's relationship. Therefore, I cannot determine whether Jeff and Eddy are neighbors or not.\n"
     ]
    }
   ],
   "source": [
    "response_llama3_8b_chat = llama(prompt,\n",
    "                        model=\"META-LLAMA/Llama-3-8B-CHAT-HF\")\n",
    "print(response_llama3_8b_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d7ea0b1-c040-4fb3-96c7-676e6f64442a",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, I can conclude that:\n",
      "\n",
      "**Jeff and Eddy are not neighbors.**\n",
      "\n",
      "Here's my reasoning:\n",
      "\n",
      "1. We know that Jeff and Tommy are neighbors.\n",
      "2. We also know that Tommy and Eddy are not neighbors.\n",
      "3. Since Tommy is a neighbor of Jeff, but not a neighbor of Eddy, it implies that Jeff and Eddy do not share a common neighbor (Tommy).\n",
      "4. Therefore, Jeff and Eddy are not neighbors.\n",
      "\n",
      "So, I can confidently answer that Jeff and Eddy are not neighbors.\n"
     ]
    }
   ],
   "source": [
    "response_llama3_70b_chat = llama(prompt,\n",
    "                        model=\"META-LLAMA/Llama-3-70B-CHAT-HF\")\n",
    "print(response_llama3_70b_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017fdb2-3e5d-4f4b-862d-acdb3dc41e5c",
   "metadata": {},
   "source": [
    "#### Model-Graded Evaluation: Reasoning\n",
    "\n",
    "- Again, ask a LLM to compare the three responses.\n",
    "- Create a `prompt` that will evaluate these three responses using 70B parameter chat model (`llama-2-70b-chat`).\n",
    "- In the `prompt`, provide the `context`, `query`,\"name of the models\", and the \"response\" generated by each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c742104-ef6d-43f6-afa0-6f8ae67ace4a",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the context `context:`,\n",
    "Also also given the query (the task): `query:`\n",
    "and given the name of several models: `mode:<name of model>,\n",
    "as well as the response generated by that model: `response:`\n",
    "\n",
    "Provide an evaluation of each model's response:\n",
    "- Does it answer the query accurately?\n",
    "- Does it provide a contradictory response?\n",
    "- Are there any other interesting characteristics of the model's output?\n",
    "\n",
    "Then compare the models based on their evaluation \\\n",
    "and recommend the models that perform the best.\n",
    "\n",
    "context: ```{context}```\n",
    "\n",
    "model: llama-2-7b-chat\n",
    "response: ```{response_7b_chat}```\n",
    "\n",
    "model: meta-llama/Llama-Vision-Free\n",
    "response: ```{response_free_chat}```\n",
    "\n",
    "model: Qwen/QwQ-32B-Preview\n",
    "response: ``{response_qwen_chat}```\n",
    "\n",
    "model: llama-3-8b-chat\n",
    "response: ```{response_llama3_8b_chat}```\n",
    "\n",
    "model: llama-3-70b-chat\n",
    "response: ``{response_llama3_70b_chat}``\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc6f1cde-68e2-4e67-96da-a98dd2e6bc45",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the evaluation of each model's response:\n",
      "\n",
      "**Llama-2-7b-chat**\n",
      "\n",
      "* Does it answer the query accurately? Yes, the model correctly concludes that Jeff and Eddy are not neighbors based on the given context.\n",
      "* Does it provide a contradictory response? No, the response is consistent with the context.\n",
      "* Are there any other interesting characteristics of the model's output? The model provides a clear and concise explanation for its answer.\n",
      "\n",
      "**Llama-2-13b-chat**\n",
      "\n",
      "* Does it answer the query accurately? No, the model's response is incomplete and does not provide an answer to the query.\n",
      "* Does it provide a contradictory response? No, the response is incomplete and does not provide any information.\n",
      "* Are there any other interesting characteristics of the model's output? The model's response is incomplete and does not provide any useful information.\n",
      "\n",
      "**Qwen/QwQ-32B-Preview**\n",
      "\n",
      "* Does it answer the query accurately? No, the model states that it does not have enough information to answer the question, which is correct given the context.\n",
      "* Does it provide a contradictory response? No, the response is consistent with the context.\n",
      "* Are there any other interesting characteristics of the model's output? The model provides a detailed and step-by-step explanation for its inability to answer the question, which is impressive.\n",
      "\n",
      "**Llama-3-8b-chat**\n",
      "\n",
      "* Does it answer the query accurately? No, the model states that it does not have enough information to answer the question, which is correct given the context.\n",
      "* Does it provide a contradictory response? No, the response is consistent with the context.\n",
      "* Are there any other interesting characteristics of the model's output? The model's response is similar to Qwen/QwQ-32B-Preview's response, but lacks the detailed explanation.\n",
      "\n",
      "**Llama-3-70b-chat**\n",
      "\n",
      "* Does it answer the query accurately? Yes, the model correctly concludes that Jeff and Eddy are not neighbors based on the given context.\n",
      "* Does it provide a contradictory response? No, the response is consistent with the context.\n",
      "* Are there any other interesting characteristics of the model's output? The model provides a clear and concise explanation for its answer, similar to Llama-2-7b-chat's response.\n",
      "\n",
      "Comparison and Recommendation:\n",
      "\n",
      "Based on the evaluation, Llama-2-7b-chat and Llama-3-70b-chat perform the best, as they both accurately answer the query and provide clear explanations for their answers. Qwen/QwQ-32B-Preview also performs well, as it correctly states that it does not have enough information to answer the question and provides a detailed explanation for its inability to do so. Llama-2-13b-chat and Llama-3-8b-chat do not perform as well, as they either do not provide an answer or lack a detailed explanation for their response.\n",
      "\n",
      "Recommended models: Llama-2-7b-chat, Llama-3-70b-chat, and Qwen/QwQ-32B-Preview.\n"
     ]
    }
   ],
   "source": [
    "response_eval = llama(prompt, \n",
    "                      model=\"META-LLAMA/Llama-3-70B-CHAT-HF\")\n",
    "\n",
    "print(response_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
